{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d880a2",
   "metadata": {},
   "source": [
    "<h3 align=\"right \">Name  : Aryan Vigyat</h3><h3 align=\"right\"> Registration Number  :  20BCE1452</h3><p style=\"color:red;font-size:25px\"> Question 1:</p>  <br><b style=\"font-size:28px\">Breadth-First-Search\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c29785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urljoin\n",
    "from urllib.request import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_intern = set()\n",
    "links_extern = set()\n",
    "input_url=input(\"Enter Link:\")\n",
    "depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_crawler(input_url):\n",
    "    temp_urls=set()\n",
    "    current_url_domain=urlparse(input_url).netloc\n",
    "    beautiful_soup_object = BeautifulSoup(\n",
    "requests.get(input_url).content, \"html.parser\")\n",
    "    for anchor in beautiful_soup_object.findAll(\"a\"):\n",
    "        href = anchor.attrs.get(\"href\")\n",
    "        if(href!=\"\" or hzref!=None):\n",
    "            href=urljoin(input_url, href)\n",
    "            href_parsed=urlparse(href)\n",
    "            href=href_parsed.scheme\n",
    "            href+=\"://\"\n",
    "            href+=href_parsed.netloc\n",
    "            href+=href_parsed.path\n",
    "            final_parsed_href=urlparse(href)\n",
    "            is_valid=bool(final_parsed_href.scheme) and bool(\n",
    "final_parsed_href.netloc)\n",
    "            if is_valid:\n",
    "                if current_url_domain not in href and href not in links_extern:\n",
    "                    print(\"External - {}\".format(href))\n",
    "                    links_extern.add(href)\n",
    "                if current_url_domain in href and href not in links_intern:\n",
    "                    print(\"Internal - {}\".format(href))\n",
    "                    links_intern.add(href)\n",
    "                    temp_urls.add(href)\n",
    "    return temp_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(depth == 0):\n",
    "    print(\"Intern - {}\".format(input_url))\n",
    "elif(depth == 1):\n",
    "    level_crawler(input_url)\n",
    "else:\n",
    "    queue = []\n",
    "    queue.append(input_url)\n",
    "    for j in range(depth):\n",
    "        for count in range(len(queue)):\n",
    "            url = queue.pop(0)\n",
    "            urls = level_crawler(url)\n",
    "            for i in urls:\n",
    "                queue.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7029cd98",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 2:</p>  <br><b style=\"font-size:21px\">Depth-First-Search\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50275d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs(base, path,visited,max_depth=3, depth=0):\n",
    "    if depth < max_depth:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(base + path).text, \"html.parser\")\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                href = link.get(\"href\")\n",
    "                if href not in visited:\n",
    "                    visited.add(href)\n",
    "                    print(f\"at depth {depth}: {href}\")\n",
    "                    if href.startswith(\"http\"):\n",
    "                        dfs(href, \"\", visited,max_depth-1, depth + 1)\n",
    "                    else:\n",
    "                        dfs(base, href,visited,max_depth-1, depth + 1)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "link=str(input(\"Input the Link to implement the DFS Technique\"))\n",
    "dfs(link, \"\", set([link]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b644433",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 3:</p>  <br><b style=\"font-size:15px\">Twitter Topic Focused Crawler – on Topic “FIFA 2022”(Challenging Task)\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e445f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINCE API OF TWITTER WASNT WORKING\n",
    "seed_url=\"https://twitter.com/search?q=FIFA%20QATAR%202022&src=typed_query\"\n",
    "response=requests.get(seed_url)\n",
    "print(\"Response Status Code :\",response.status_code)\n",
    "rootPage=soup(response.content,'html.parser')\n",
    "atags=rootPage.find_all('a')\n",
    "for atag in atags:\n",
    "    link=atag['href']\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f8fa7",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 4:</p>  <br><b style=\"font-size:15px\">Identify the Robot exclusion protocol file in your preferred website. Robots Exclusion protocol is used to tell search engine crawlers which URLs it should NOT request when crawling a Web site.\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc03ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "seedURL = input(\"Enter the Input URL:\") \n",
    "if(seedURL.endswith('/')):\n",
    "    path=seedURL\n",
    "else:\n",
    "    path=seedURL+'/'\n",
    "response = requests.get(path+\"robots.txt\",data=None) \n",
    "print(\"Status of the response : \", response.status_code) \n",
    "rootPage=soup(response.content,'html.parser')\n",
    "print(\"-----------Result----------\")\n",
    "print(rootPage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aefb8f425c96f33eba64b27e340694185e828f19562f8f40b86ace482095f222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
