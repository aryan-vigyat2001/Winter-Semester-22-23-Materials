{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a0900b",
   "metadata": {},
   "source": [
    "<h2 align=\"right \">Name  : Aryan Vigyat</h2><h2 align=\"right\"> Registration Number  :  20BCE1452</h2><p style=\"color:red;font-size:25px\"> Question 1:</p>  <br><b style=\"font-size:18px\">Given a seed/root URL, e.g., \"Vit.ac.in\", Design a simple crawler to return all pages (URLs) that contains a keyword \"research\" from this site. (25 pages) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4b7d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code : 200\n",
      "The Links in the SEED URL Page which consists of the word research are as follows\n",
      "\t https://vit.ac.in/admissions/research\n",
      "\t https://vit.ac.in/research\n",
      "\t https://vit.ac.in/research\n",
      "\t https://vit.ac.in/research/academic\n",
      "\t https://vit.ac.in/research/sponsored-research\n",
      "\t https://vit.ac.in/research/centers-list\n",
      "\t https://vit.ac.in/vit-business-school-vit-bs/five-days-workshop-data-analysis-business-and-management-research-online\n",
      "\t https://vit.ac.in/vit-business-school-vit-bs/five-days-workshop-data-analysis-business-and-management-research-online\n",
      "\t https://vit.ac.in/schools-centres-list-research-guides-2022\n",
      "\t 3d-printing-play-major-role-mitigating-spread-covid-19-say-researchers-vit\n",
      "\t 3d-printing-play-major-role-mitigating-spread-covid-19-say-researchers-vit\n",
      "\t https://vit.ac.in/research\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "seed_url=\"http://www.vit.ac.in\"\n",
    "searchWord=\"research\"\n",
    "maxPages=25\n",
    "response=requests.get(seed_url)\n",
    "print(\"Response Status Code :\",response.status_code)\n",
    "rootPage=soup(response.content,'html.parser')\n",
    "atags=rootPage.find_all('a')\n",
    "result=list()\n",
    "pages=0\n",
    "for atag in atags:\n",
    "    if(pages==maxPages):\n",
    "        break\n",
    "    link=atag['href']\n",
    "    if re.search(searchWord,link,re.IGNORECASE):\n",
    "        result.append(link)\n",
    "        pages+=1\n",
    "        \n",
    "print(\"The Links in the SEED URL Page which consists of the word research are as follows\")\n",
    "for res in result:\n",
    "    print(\"\\t\",res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154afc9",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 2:</p>  <br><b style=\"font-size:18px\">Find documents that contain the word “admissions” and the word “international” within the URL “Vit.ac.in” using Python. (25 pages) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46adaef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Status Code : 200\n",
      "Total Number of Documents is 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayuar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'chennai.vit.ac.in'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re \n",
    "seed_URL = \"http://www.vit.ac.in\" \n",
    "searchWords = ['admissions', 'international']\n",
    "maxPages=25\n",
    "response=requests.get(seed_URL)\n",
    "print(\"Response Status Code :\",response.status_code)\n",
    "rootPage=soup(response.content,'html.parser')\n",
    "atags=rootPage.find_all('a')\n",
    "validLinks=list()\n",
    "for atag in atags:\n",
    "    link=atag['href']\n",
    "    if link.startswith(\"http\") : \n",
    "        if link not in validLinks : \n",
    "            validLinks.append(link) \n",
    "print(\"Total Number of Documents is {}\".format(len(validLinks)))\n",
    "final=list()\n",
    "foundPages=0\n",
    "failed=list()\n",
    "pages=0\n",
    "for link in validLinks : \n",
    "    if(pages==maxPages):\n",
    "        break\n",
    "    try : \n",
    "        page = requests.get(link).text \n",
    "    except requests.ConnectionError :\n",
    "        try : \n",
    "            page = requests.get(link, verify=False).text \n",
    "        except : \n",
    "            failed.append(link) \n",
    "        continue \n",
    "    if (re.search(searchWords[0], page, re.IGNORECASE)) and (re.search(searchWords[1], page, re.IGNORECASE)) : \n",
    "        final.append(link)\n",
    "        foundPages+=1\n",
    "    pages+=1\n",
    "print(\"The Documents that Contain the Words Admission and International is \")\n",
    "for i in final:\n",
    "    print(\"\\t\",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55ea07",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 3:</p>  <br><b style=\"font-size:18px\">Find documents that contain the word “Programme” but not the word “programming” within the URL “Vit.ac.in” using Python. (5 pages) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "seedURL=\"http://vit.ac.in\"\n",
    "searchWords=['Programme','Programming']\n",
    "response=requests.get(seedURL)\n",
    "maxPages=5\n",
    "print(\"Response Status Code :\",response.status_code)\n",
    "rootPage=soup(response.content,'html.parser')\n",
    "atags=rootPage.find_all('a')\n",
    "validLinks=list()\n",
    "for atag in atags:\n",
    "    link=atag['href']\n",
    "    if link.startswith(\"http\") : \n",
    "        if link not in validLinks : \n",
    "            validLinks.append(link) \n",
    "print(\"Total Number of Documents Linked to the Root URL is {}\".format(len(validLinks)))\n",
    "final=list()\n",
    "foundPages=0\n",
    "failed=list()\n",
    "pages=0\n",
    "for link in validLinks : \n",
    "    if(pages==maxPages):\n",
    "        break\n",
    "    try : \n",
    "        page = requests.get(link).text \n",
    "    except requests.ConnectionError :\n",
    "        try : \n",
    "            page = requests.get(link, verify=False).text \n",
    "        except : \n",
    "            failed.append(link) \n",
    "        continue \n",
    "    if (re.search(searchWords[0], page, re.IGNORECASE)) and (not re.search(searchWords[1], page, re.IGNORECASE)) : \n",
    "        final.append(link)\n",
    "        foundPages+=1\n",
    "    pages+=1\n",
    "print(\"The Documents that Contain the Word Programme and not Programming is \")\n",
    "for i in final:\n",
    "    print(\"\\t\",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e866f44",
   "metadata": {},
   "source": [
    "<p style=\"color:red;font-size:25px\"> Question 4:</p>  <br><b style=\"font-size:18px\">Write a web crawler program which takes as input a url (Educational website) and a search key word and maximum number of pages (15-20 Pages)  to be searched and returns as output all the web pages it searched till it found the search word on a web page or return failure. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "seedURL4 = input(\"Enter the Input URL:\") \n",
    "searchWord = input(\"Enter the Search Word: \") \n",
    "maxPages = int(input(\"Enter the Max Pages:\")) \n",
    "response = requests.get(seedURL4) \n",
    "print(\"Status of the response : \", response.status_code) \n",
    "rootPage=soup(response.content,'html.parser')\n",
    "atags=rootPage.find_all('a')\n",
    "validLinks=list()\n",
    "for atag in atags:\n",
    "    try:\n",
    "        link=atag['href']\n",
    "        if link.startswith(\"http\") : \n",
    "            if link not in validLinks : \n",
    "                validLinks.append(link) \n",
    "    except:\n",
    "        pass\n",
    "print(\"Total Number of Documents is {}\".format(len(validLinks)))\n",
    "final=list()\n",
    "foundPages=0\n",
    "failed=list()\n",
    "pages=0\n",
    "for link in validLinks : \n",
    "    if(pages==maxPages):\n",
    "        break\n",
    "    try : \n",
    "        page = requests.get(link).text \n",
    "    except requests.ConnectionError :\n",
    "        try : \n",
    "            page = requests.get(link, verify=False).text \n",
    "        except : \n",
    "            failed.append(link) \n",
    "        continue \n",
    "    if (re.search(searchWord, page, re.IGNORECASE)):\n",
    "        final.append(link)\n",
    "        foundPages+=1\n",
    "    pages+=1\n",
    "if(foundPages==0):\n",
    "    print(\"Failure\")\n",
    "else:\n",
    "    print(\"The Documents that Contain the Word {} is \".format(searchWord))\n",
    "    for i in final:\n",
    "        print(\"\\t\",i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8afaaf",
   "metadata": {},
   "source": [
    "<br><p style=\"color:red;font-size:30px\"> Question 5:</p>  <br><b style=\"font-size:18px\">Write a Python program to read the given website and extract the phone numbers and emails and contact addresses from Chennai, Amaravathi, Bhopal vit website. (5 Marks) </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "seedURL5=[\"https://vitap.ac.in/\",\"https://vitbhopal.ac.in/\",\"https://chennai.vit.ac.in/\"]\n",
    "f = open(\"myfile.txt\", \"w\")\n",
    "for url in seedURL5:\n",
    "    response = requests.get(url,verify=False) \n",
    "    print(\"Status of the response : \", response.status_code) \n",
    "    rootPage=soup(response.content,'html.parser')\n",
    "    phpattern = re.compile(r'[7-9][0-9]{9}')\n",
    "    emailpattern=re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    phone_numbers=re.findall(phpattern,str(rootPage))\n",
    "    email_ids=re.findall(emailpattern,str(rootPage))\n",
    "    f.write(\"-------{} Email IDS and Phone Number-------\\n\".format(url))\n",
    "    f.write((' '.join(email_ids)))\n",
    "    f.write((' '.join(phone_numbers)))\n",
    "    f.write(\"\\n\")\n",
    "f.close()\n",
    "f2 = open(\"myfile.txt\", \"r\")\n",
    "print(f2.read())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aefb8f425c96f33eba64b27e340694185e828f19562f8f40b86ace482095f222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
