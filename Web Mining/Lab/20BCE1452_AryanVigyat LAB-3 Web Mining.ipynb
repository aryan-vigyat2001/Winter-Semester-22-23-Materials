{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Web Mining Lab Assignment-3</h1>\n",
    "                                                                                                                                        <h3 align=\"right\">Aryan Vigyat</h3>\n",
    "                                                                                                            <h3 align=\"right\"> 20BCE1452</h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\n",
    "    \"Selenium is a portable framework for testing web applications\",\n",
    "    \"Beautiful Soup is useful for web scraping\",\n",
    "    \"It is a python package for parsing the pages\",\n",
    "    \"Java programming can be used for web applications\",\n",
    "    \"scraping web and crawling web is useful\"\n",
    "]\n",
    "d=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc,indx):\n",
    "    doc=doc.lower()\n",
    "    word_tokens = word_tokenize(doc)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in range(0,len(word_tokens)):\n",
    "        if(word_tokens[i].lower() in stop_words or word_tokens[i].isnumeric()):\n",
    "            continue\n",
    "        if(word_tokens[i]  in d.keys()):\n",
    "            x=d[word_tokens[i]]\n",
    "            x.append([indx+1,i])\n",
    "            d[word_tokens[i]]=x\n",
    "        else:\n",
    "            x=[[indx+1,i]]\n",
    "            d[word_tokens[i]]=x\n",
    "\n",
    "\n",
    "\n",
    "    # filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words and not w.isnumeric()]\n",
    "    # for w in range(0,len(filtered_sentence)):\n",
    "    #     if(filtered_sentence[w] in d.keys()):\n",
    "    #         x=d[filtered_sentence[w]]\n",
    "    #         x.append([indx+1,w])\n",
    "    #         d[filtered_sentence[w]]=x\n",
    "    #     else:\n",
    "    #         x=[[indx+1,w]]\n",
    "    #         d[filtered_sentence[w]]=x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(docs)):\n",
    "    preprocess(docs[i],i)\n",
    "print(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium and Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"Selenium\"\n",
    "y=\"Web\"\n",
    "x=x.lower()\n",
    "y=y.lower()\n",
    "val1=d[x]\n",
    "val2=d[y]\n",
    "for (i,j),(k,l) in zip(val1,val2):\n",
    "    if(docs[i].lower().find(y)!=-1):\n",
    "        print(\"Occurs in Document {} at offset {}\".format(i,j))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"Soup\"\n",
    "x=x.lower()\n",
    "if(x not in d.keys()):\n",
    "    pass\n",
    "else:\n",
    "    val=d[x]\n",
    "    for i,j in val:\n",
    "        print(\"Occurs in Document {} at offset {}\".format(i,j))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python OR Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"Python\"\n",
    "y=\"Java\"\n",
    "x=x.lower()\n",
    "y=y.lower()\n",
    "if(x not in d.keys() and y not in d.keys()):\n",
    "    pass\n",
    "else:\n",
    "    res=d[x]+d[y]\n",
    "    for i,j in res:\n",
    "        print(\"Occurs in Document {} at offset {}\".format(i,j))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web AND craw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"Craw\"\n",
    "y=\"Web\"\n",
    "if(x not in d.keys() and y not in d.keys()):\n",
    "    pass\n",
    "else:\n",
    "    x=x.lower()\n",
    "    y=y.lower()\n",
    "    val1=d[x]\n",
    "    val2=d[y]\n",
    "    for (i,j),(k,l) in zip(val1,val2):\n",
    "        if(docs[i].lower().find(y)!=-1):\n",
    "            print(\"Occurs in Document {} at offset {}\".format(i,j))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "docs=[\"Information Retrieval Systems is used with database systems\",\n",
    "\"Information is in Storage\",\n",
    "\"Digital Speech can be used in Synthesis and Systems\",\n",
    "\"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\",\n",
    "\"Database Management system is used for storage\"\n",
    "]\n",
    "d=dict()\n",
    "cnt=0\n",
    "d1=dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc,indx):\n",
    "    doc=doc.lower()\n",
    "    word_tokens = word_tokenize(doc)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    l=list()\n",
    "    for i in range(0,len(word_tokens)):\n",
    "        if(word_tokens[i]==',' or word_tokens[i].lower() in stop_words or word_tokens[i].isnumeric()):\n",
    "            continue\n",
    "        else:\n",
    "            l.append(word_tokens[i].lower())\n",
    "    return (\" \").join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(docs)):\n",
    "    docs[i]=preprocess(docs[i],i)\n",
    "terms = list(set([word for doc in docs for word in doc.split(\" \")]))\n",
    "print(terms)\n",
    "matrix = [[1 if term in doc.split(\" \") else 0 for term in terms] for doc in docs]\n",
    "print(matrix)\n",
    "df = pd.DataFrame(matrix, columns=terms, index=[\"Doc 1\", \"Doc 2\", \"Doc 3\", \"Doc 4\", \"Doc 5\"])\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the documents for the Boolean query “Information Retrieval\n",
    "Synthesis” using simple match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Information Retrieval Synthesis\"\n",
    "query=query.lower().split(\" \")\n",
    "result = df[(df[query[0]] == 1) & (df[query[1]] == 1) &(df[query[2]]==1)]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the documents for the Boolean query “Database Retrieval\n",
    "Storage” using weighted match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\"database\": 2, \"retrieval\": 1, \"storage\": 3} #Assigning Weights to the terms\n",
    "for term, weight in weights.items():\n",
    "    df[term + \"_weight\"] = df[term].apply(lambda x: weight if x==1 else 0)\n",
    "df[\"score\"] = df[[\"database_weight\", \"retrieval_weight\", \"storage_weight\"]].sum(axis=1)\n",
    "result = df.sort_values(\"score\", ascending=False)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "docs=[\"Information Retrieval Systems is used with database systems\",\n",
    "\"Information is in Storage\",\n",
    "\"Digital Speech can be used in Synthesis and Systems\",\n",
    "\"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\",\n",
    "\"Database Management system is used for storage\"\n",
    "]\n",
    "d=dict()\n",
    "cnt=0\n",
    "d1=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc,indx):\n",
    "    doc=doc.lower()\n",
    "    word_tokens = word_tokenize(doc)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    l=list()\n",
    "    for i in range(0,len(word_tokens)):\n",
    "        if(word_tokens[i]==',' or word_tokens[i].lower() in stop_words or word_tokens[i].isnumeric()):\n",
    "            continue\n",
    "        else:\n",
    "            l.append(word_tokens[i].lower())\n",
    "    return (\" \").join(l)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(docs)):\n",
    "    docs[i]=preprocess(docs[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(docs)):\n",
    "    y=docs[i].split(\" \")\n",
    "    for j in y:\n",
    "        if j in d.keys():\n",
    "            pass\n",
    "        else:\n",
    "            d[j]=cnt\n",
    "            cnt+=1\n",
    "d= dict(sorted(d.items()))\n",
    "n1=0\n",
    "for i in d.keys():\n",
    "    d[i]=n1\n",
    "    n1+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(docs)):\n",
    "    d1[i+1]=[0]*len(d.keys())\n",
    "temp=list()\n",
    "for i in range(0,len(docs)):\n",
    "    y=docs[i].split(\" \")\n",
    "    for j in y:\n",
    "        d1[i+1][d[j]]+=1\n",
    "org=copy.deepcopy(list(d1.values()))\n",
    "tf=list(d1.values())\n",
    "for i in range(0,len(d.keys())):\n",
    "    c_nt=0\n",
    "    for j in range(0,len(docs)):\n",
    "        if tf[j][i]>=1:\n",
    "            c_nt+=tf[j][i]\n",
    "    temp.append(c_nt)\n",
    "for i in range(0,len(temp)):\n",
    "    temp[i]=round(math.log2(len(docs)/temp[i]),3)\n",
    "for i in range(0,len(docs)):\n",
    "    for j in range(0,len(d.keys())):\n",
    "        tf[i][j]=round(tf[i][j]/len(docs[i].split(\" \")),2)\n",
    "tf_idf=tf\n",
    "for i in range(0,len(docs)):\n",
    "    for j in range(0,len(d.keys())):\n",
    "        tf_idf[i][j]=round(tf_idf[i][j]*temp[j],3)\n",
    "print(\"The TF-IDF Scores are :-\")\n",
    "print(tf_idf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank all the documents in the collection for the query “Speech\n",
    "Systems”? (Rank the documents in the order of relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[0]*len(d.items())\n",
    "str=\"Speech System\"\n",
    "str=str.lower()\n",
    "temp1=set(str.split(\" \"))\n",
    "for i in temp1:\n",
    "    l[d[i]]=str.count(i)/len(str.split(\" \"))\n",
    "stdict=dict()\n",
    "for i in range(0,len(tf_idf)):\n",
    "    vector1=tf_idf[i]\n",
    "    vector2=l\n",
    "    ans=np.dot(vector1,vector2)\n",
    "    stdict[i]=ans\n",
    "stdict=dict(sorted(stdict.items(), key=lambda item: item[1]))\n",
    "print(\"The rank starting from top to bottom is :-\")\n",
    "for i,j in reversed(stdict.items()):\n",
    "    print(\"Document {} \".format(i+1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Cosine Similarity for Docs1 and Docs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_1=np.array(tf_idf[0])\n",
    "tf_2=np.array(tf_idf[1])\n",
    "res=np.multiply(tf_1,tf_2)\n",
    "if(sum(res)==0):\n",
    "    print(\"No similarity between the documents\")\n",
    "else:\n",
    "    tf_1sq=sum(np.multiply(tf_1,tf_1))\n",
    "    tf_2sq=sum(np.multiply(tf_2,tf_2))\n",
    "    ans=sum(res)/(math.sqrt(tf_1sq)*math.sqrt(tf_2sq))\n",
    "    print(ans)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Dice Co-efficient between docs 3 and docs 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1=np.array(tf_idf[2])\n",
    "vector2=np.array(tf_idf[3])\n",
    "intersection = np.dot(vector1, vector2)\n",
    "ans= (2. * intersection) / (np.linalg.norm(vector1) ** 2 + np.linalg.norm(vector2) ** 2)\n",
    "print(ans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Jaccard co-efficient between docs 4 and docs 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1=np.array(tf_idf[3])\n",
    "vector2=np.array(tf_idf[4])\n",
    "dot_product = np.dot(vector1, vector2)\n",
    "norm1 = np.linalg.norm(vector1, ord=1)\n",
    "norm2 = np.linalg.norm(vector2, ord=1)\n",
    "ans= dot_product / (norm1 + norm2 - dot_product)\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aefb8f425c96f33eba64b27e340694185e828f19562f8f40b86ace482095f222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
